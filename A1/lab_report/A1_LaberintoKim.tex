\documentclass{article}

\usepackage{amsfonts}
\usepackage{amsmath}

\addtolength{\jot}{1em} %https://tex.stackexchange.com/questions/14679/amsmath-align-environment-row-spacing

\title{Optimization Assignment 1}
\date{2021-02-xx}
\author{Kim Paolo Laberinto}

\begin{document}
    \maketitle
    \newpage

    \tableofcontents
    \newpage

    \section{Q1.}

    \subsection{Example 1D Optimization Trajectories}

    %Play around with different starting points and directions
    %Show trajectory in 1D show trajectory in 2D

    %Incorrectly found examples
    %Correctly found examples changing interval size

    \subsection{Comparision between Swann's and Powell's Methods}

    \section{Q2.}

    \subsection{Example Optimization Trajectories}

    \subsection{Example 1D Optimizations in Trajectory}

    \subsection{(EXTRA) Hooke-Jeeves Result on Rosenbrock}

    \section{Q3.}

    \subsection{Gradient Descent}

    \subsubsection{Objective Function Definition and Gradient Derivation}

    \begin{align}
    L &= \sum_i (y_{\text{data}} - y_{\text{model}})^2 \\
    L &= \sum_i (y_{i} - x_1 + x_2 e ^ {x_4 t_i} + x_3 e ^{x_5 t_i})^2
    \end{align}

    We can now derive the gradient.
    \begin{align}
    \nabla L &= \nabla \left( \sum_i (y_{i} - x_1 + x_2 e ^ {x_4 t_i} + x_3 e ^{x_5 t_i})^2 \right) \\
    \nabla L &= \sum_i \left( \nabla \left( (y_{i} - x_1 + x_2 e ^ {x_4 t_i} + x_3 e ^{x_5 t_i})^2 \right) \right) \\
    \nabla L &= \sum_i \left( 2(y_{i} - x_1 + x_2 e ^ {x_4 t_i} + x_3 e ^{x_5 t_i}) \cdot \nabla (y_{i} - x_1 + x_2 e ^ {x_4 t_i} + x_3 e ^{x_5 t_i}) \right) \\
    \nabla L &= \sum_i \left( 2(y_{i} - x_1 + x_2 e ^ {x_4 t_i} + x_3 e ^{x_5 t_i}) \cdot \nabla (x_1 + x_2 e ^ {x_4 t_i} + x_3 e ^{x_5 t_i}) \right)
    \end{align}

    \begin{align}
        \nabla (x_1 + x_2 e ^ {x_4 t_i} + x_3 e ^{x_5 t_i}) = 
            \begin{bmatrix}
                1 \\
                e ^ {x_4 t_i} \\
                e ^ {x_5 t_i} \\
                x_2 t_i e ^ {x_4 t_i} \\
                x_3 t_i e ^{x_5 t_i} \\
            \end{bmatrix}
    \end{align}

    \subsection{Hooke-Jeeves}

    \subsection{(EXTRA) Automatic Differentiation Gradient Descent}

    \section{Q4. (Graduate Student)}

    Let $V$ be the vector space of polynomial functions from $\mathbb{R} \rightarrow \mathbb{R}$ which have degree less than or equal to 2.

    That is, for all $a, b, c \in \mathbb{R}$ the following polynomial $p(x)$ is in $V$
    
    \begin{align*}
    p(x) = a + bx + cx^2
    \end{align*}    

    \subsection{Proving $B_1$ and $B_1$ are both a basis of $V$}

    \subsubsection{Proof: $B_1$ is Linearly Independent}
    - Implication of only way to get 0 vector.
    \subsubsection{Proof: $B_1$ spans $V$}
    - Show all $v \in V$ can be represented in basis

    \subsubsection{Proof: $B_2$ is Linearly Independent}

    \subsubsection{Proof: $B_2$ spans $V$}

    \subsection{Transformation between $B_1$ and $B_2$}

    \subsection{D Derivative Operator}

    \subsubsection{Proof: D is linear}

    \subsubsection{Matrix Representation of D in the bases $B_1$ and $B_2$}

    \section{Q5. (Graduate Student)}

    \subsection{x in full $\mathbb{R}^3$ standard basis}
    \subsection{Coordinates of $x$ in $B_X$}
    \subsection{Coordinates of $y$ in $B_Y$ using $L$ ($c_y = Lc_x$)}
    \subsection{y in full $\mathbb{R}^4$ standard basis}


    \section{Q6. (Graduate Student)}
    \subsection{ $Df$ derivation }
    Let $f: \mathbb{R}^4 \rightarrow \mathbb{R}$ be defined as $f(x) = \| Ax - b \| + \lambda \| Cx \| + \gamma \| Eb \|$ where $A, C, E \in \mathbb{R}^{4 \times 4}$, $x,b \in \mathbb{R}^4$ and $\lambda,\gamma \in \mathbb{R}$.

    Let $D$ denote the differential operator.

    \begin{align*}
    Df(x) &= D \left( \| Ax - b \| + \lambda \| Cx \| + \gamma \| Eb \| \right) &\\
    &= D \left( \| Ax - b \| \right) +  \lambda D \left( \| Cx \| \right) +  \gamma D \left( \| Eb \| \right) & \parbox[c]{0.4\linewidth}{[using linearity of D]}\\
    &= D \left( \| Ax - b \| \right) +  \lambda D \left( \| Cx \| \right) +  0 &\\
    &= D \left( \| Ax - b \| \right) +  \lambda D \left( \| Cx \| \right) &\\
    &= \frac{ (Ax-b)^T A }{\| Ax-b \| } + \lambda \frac{(Cx)^T C}{ \| Cx \| } & \parbox[c]{0.4\linewidth}{[using derivations from next sections]}
    \end{align*}

    The full derivation of $D \left( \| Ax - b \| \right)$ and $ D \left( \| Cx \| \right) $ can be found in the following sections.

    \subsection{ $D \left(\| Cx \| \right)$ Derivation }

    \begin{align*}
        D\left( \| Cx \| \right) &= D \left( \sqrt{(Cx)^T (Cx)} \right) \\
        &= D \left( \left( (Cx)^T (Cx) \right)^\frac{1}{2} \right) \\
        &= \frac{1}{2} \left( (Cx)^T (Cx) \right)^\frac{-1}{2} D \left( (Cx)^T (Cx) \right) & \parbox[c]{0.4\linewidth}{[using Chain Rule]}\\
        &= \frac{1}{2 \sqrt{ (Cx)^T (Cx) } }  D \left( (Cx)^T (Cx) \right) \\
        &= \frac{1}{2 \| Cx \| }  D \left( (Cx)^T (Cx) \right) \\
        &= \frac{\left( (Cx)^T D(Cx) \right) + \left( (Cx)^T D(Cx) \right) }{2 \| Cx \| } & \parbox[c]{0.4\linewidth}{[using Product Rule]}\\
        &= \frac{2 \left( (Cx)^T D(Cx) \right)}{2 \| Cx \| } \\
        &= \frac{(Cx)^T C}{ \| Cx \| }
    \end{align*}

    \subsection{ $D \left( \| Ax-b \| \right)$ Derivation}

    \begin{align*}
        D\left( \| Ax-b \| \right) &= D \left( \sqrt{(Ax-b)^T (Ax-b)} \right) \\
        &= D \left( \left( (Ax-b)^T (Ax-b) \right)^{\frac{1}{2}} \right) \\
        &= \frac{1}{2} \left((Ax-b)^T (Ax-b) \right)^{\frac{-1}{2}} D \left( (Ax-b)^T (Ax-b) \right) & \parbox[c]{0.4\linewidth}{[using Chain Rule]} \\
        &= \frac{1}{2 \sqrt { (Ax-b)^T (Ax-b) }} D \left( (Ax-b)^T (Ax-b) \right) \\
        &= \frac{1}{2 \| Ax-b \| } D \left( (Ax-b)^T (Ax-b) \right) \\
        &= \frac{ \left( (Ax-b)^T D(Ax-b) \right) + \left( (Ax-b)^T D(Ax-b) \right) }{2 \| Ax-b \| } & \parbox[c]{0.4\linewidth}{[using Product Rule]} \\
        &= \frac{ 2(Ax-b)^T D(Ax-b) }{2 \| Ax-b \| } \\
        &= \frac{ 2(Ax-b)^T A }{2 \| Ax-b \| } \\
        &= \frac{ (Ax-b)^T A }{\| Ax-b \| }
    \end{align*}

    \appendix
    \section{Source Code}

    \subsection{All Optimization Algorithms (A1Module.jl)}

    \subsection{Plot Generation Script (makeplots.jl)}

    \subsection{(EXTRA) Automatic Differentiation for Gradient}

    

\end{document}